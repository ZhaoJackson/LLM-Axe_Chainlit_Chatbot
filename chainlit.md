# Llama3 Copilot ChatWindow with LLM-Axe Integration

## Overview
1. Use **LLM-Axe** to search for the latest information from the internet.
2. Pass the retrieved data to **Llama3 (Ollama)** to generate a final response while preserving context.
3. Display both the fetched data and Llama3’s contextual insights to the user.

---

## Features

1. **Chat Window Interface**: 
   - Built using **Chainlit** to allow seamless interaction where users input queries.
   
2. **Internet Search with LLM-Axe**: 
   - Uses **LLM-Axe (OnlineAgent)** to fetch updated information from the internet based on user queries.
   
3. **Contextual Response with Llama3**: 
   - **Llama3 (via Ollama)** processes the retrieved data and generates a thoughtful response or insights based on the query context.
   
4. **Real-Time Interaction**: 
   - The latest info and Llama3’s analysis are displayed to the user in real-time within the chat window.

---

## Requirements

- **Python 3.9**
- **Chainlit**
- **LLM-Axe**
- **Ollama**

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/yourusername/branchname.git
   ```

2. **Create a virtual environment**:
   ```bash
   python -m venv venv
   source venv/bin/activate
   ```

3. **Install required dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Install Chainlit, LLM-Axe, and Ollama**:
   ```bash
   pip install chainlit llm-axe ollama
   ```

---

## Setup and Usage

1. **Run the ChatWindow**:
   To launch the Chainlit-powered chat window, run:
   ```bash
   chainlit run app.py
   ```

2. **Access the Chat Interface**:
   After running the app, open your browser and go to `http://localhost:8000` to interact with the Copilot ChatWindow.

3. **Interact with the Chat**:
   - Enter any query (e.g., "latest in AI", "iPhone updates") in the chat window.
   - The app will:
     1. Fetch the latest updates using **LLM-Axe**.
     2. Pass the information to **Llama3**, which will generate insights or responses.
     3. Display both the raw data and the generated response in the chat.

---

## Code Explanation

### 1. **LLM-Axe Integration**:
   The **OnlineAgent** from **LLM-Axe** is used to search the internet for updated information related to the user’s query.

   ```python
   searcher = OnlineAgent(llm=llm)
   latest_info = searcher.search(search_prompt)
   ```

### 2. **Llama3 Integration (Ollama)**:
   The output from **LLM-Axe** is passed to **Llama3** via **Ollama** to generate contextual insights based on the query.

   ```python
   blog_response = ollama.chat(model="llama3:instruct", messages=[{"role": "user", "content": response_prompt}])
   ```

### 3. **Display to User**:
   Both the raw data from **LLM-Axe** and the insights generated by **Llama3** are displayed in the Chainlit chat window.